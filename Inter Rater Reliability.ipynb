{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667ace0e-045b-4eda-9ecb-7b7fb91a5e08",
   "metadata": {},
   "source": [
    "## Inter Rater Reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef3d62-e9f4-4f8d-b1e6-6f0b6ce2324b",
   "metadata": {},
   "source": [
    "- In stat, it is the degree of agreement among independent raters or observers who rate, code, or assess the same phenomenon.\n",
    "- Known by several names: **inter-rater agreement**, **inter-rater concordance**, **inter-observer reliability**, **inter-coder reliability** and so on.\n",
    "- Measurement of the consistency of a single study or research by different raters/observers.\n",
    "- A high **IRR** value indicates consistency of measurement across different observers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b464e-4237-4501-b751-667b5075a03c",
   "metadata": {},
   "source": [
    "#### Determining IRR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57198b-ba3e-4bac-9bb1-6fd753eea7e9",
   "metadata": {},
   "source": [
    "There are number of stats that can be used to determine inter-rater reliability, they are:\n",
    "1) **Cohen's Kappa**:\n",
    "\t- Denoted by lowercase Greek kappa, $κ$ \n",
    "\t- Measures inter-rater reliability (also intra-rater reliability) for categorical data.\n",
    "\t- Takes into account the **possibility of agreement occurring by chance**.\n",
    "\t- It measures the agreement between two raters who each classify $N$ items into $C$  mutually exclusive categories.\n",
    "\t- The definition of $κ$ is:\n",
    "\t$${\\displaystyle \\kappa \\equiv {\\frac {p_{o}-p_{e}}{1-p_{e}}}=1-{\\frac {1-p_{o}}{1-p_{e}}}}$$\n",
    "\n",
    "\t\twhere, <br />\n",
    "\t\t$p_{o}$ = the relative observed agreement among raters<br />\n",
    "\t\t$p_{e}$ = the hypothetical probability of chance agreement, using the observed data to calculate the prob. of each observer randomly seeing each category.\n",
    "\n",
    "\t\t**Note**: If raters are in complete agreement $κ = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77ab2a-679e-4a39-833a-bae0b93a8709",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "To analyze the inter-rater reliability using Cohen's Kappa in Python for a dataset where two readers (A and B) evaluated grant proposals with \"Yes\" or \"No\" decisions, we can follow these steps:\n",
    "\n",
    "- Create a **confusion matrix** (contingency table) to represent the agreement and disagreement counts.\n",
    "\n",
    "  What is a **confusion matrix**?<br />\n",
    "  In the field of machine learning and specifically the problem of statistical classification, a confusion matrix (also known as error matrix) is a specific table layout that allows visualization of the performance of an algorithm. The name (confusion) stems from the fact that it makes it easy to see whether the system or model is confusing two classes (i.e. commonly mislabeling one as another).\n",
    "\n",
    "- Compute Cohen's Kappa score based on this confusion matrix.\n",
    "\n",
    "  Assume the following confusion matrix where:\n",
    "\n",
    "    - a is the count of \"Yes-Yes\" agreements.\n",
    "    - b is the count of \"Yes-No\" disagreements.\n",
    "    - c is the count of \"No-Yes\" disagreements.\n",
    "    - d is the count of \"No-No\" agreements.\n",
    " \n",
    "|                 | Reader B: Yes | Reader B: No |\n",
    "|-----------------|---------------|--------------|\n",
    "| Reader A: Yes   | a = 20        | b = 5        |\n",
    "| Reader A: No    | c = 10        | d = 15       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82660598-1e0c-48cd-9a57-5628482aef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e3ba0fd-13e6-4f39-92c3-be254404923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: Create the confusion matrix\n",
    "We'll first convert this matrix into individual ratings for each proposal\n",
    "Suppose we have 20 \"Yes-Yes\", 5 \"Yes-No\", 10 \"No-Yes\", and 15 \"No-No\"\n",
    "\"\"\"\n",
    "reader_a = np.array([1]*20 + [1]*5 + [0]*10 + [0]*15) # 1 = Yes and 0 = No\n",
    "reader_b = np.array([1]*20 + [0]*5 + [1]*10 + [0]*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72c5ce3-0333-4265-843f-48f4ac486b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the matrix\n",
    "reader_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "547e4c8e-3fa0-479d-9556-e544fe479376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cohen kappa score between reader_a and reader_b\n",
    "kappa_score = cohen_kappa_score(reader_a, reader_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27c2e128-6264-48db-ba1b-c6c5a0666e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Cohen's Kappa: 0.40000\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Cohen's Kappa: {kappa_score:.5f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613827f-0b97-42c4-8a4b-9a34d90af230",
   "metadata": {},
   "source": [
    "It indicates that the degree of agreement between the two readers beyond chance. A kappa value of 0.400 indicates moderate level of agreement between the readers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aeed43-8482-495a-9598-2acd1a74c446",
   "metadata": {},
   "source": [
    "## Fleiss' Kappa\n",
    "\n",
    "- Extension of Cohen's kappa for multiple raters.\n",
    "- It can be used to assess reliability of agreement between a fixed number of raters when classifying items.\n",
    "- It measures the degree of agreement in classification over that which would be expected by chance.\n",
    "- It can be thought of as: if a fixed number of people assign numerical ratings to a number of items then the kappa will give a measure for how consistent the ratings are.\n",
    "\n",
    "- The definition of $\\kappa$ is:\n",
    "\n",
    "$${\\displaystyle \\kappa ={\\frac {{\\bar {P}}-{\\bar {P_{e}}}}{1-{\\bar {P_{e}}}}}}$$\n",
    "\n",
    "<br />\n",
    "\n",
    "where, <br />\n",
    "${\\displaystyle 1-{\\bar {P_{e}}}}$ = the factor that gives the degree of agreement attainable above chance <br />\n",
    "${\\displaystyle {\\bar {P}}-{\\bar {P_{e}}}}$ = the degree of agreement actually achieved above chance\n",
    "\n",
    "**Note**: If raters are in complete agreement, $\\kappa = 1$. If there is no agreement among the raters (other than what would      be expected by chance), then ${\\displaystyle \\kappa \\leq 0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1910e1-8b02-43d4-b2cd-31fb32f8d015",
   "metadata": {},
   "source": [
    "#### Example Data:\n",
    "\n",
    "Let's assume we have 50 proposals rated by 3 raters (A, B, C) with \"Yes\" (1) or \"No\" (0). How do we calculate the Fleiss' Kappa?\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Create a rating matrix where rows represent proposals and columns represent raters.\n",
    "- Use `statsmodel` library to compute Fleiss' Kappa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fb84145-28f8-4c20-9b11-6fc6e0d76e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e7f9d83-e4d0-4526-8c61-8a508cac702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example data: 50 proposals rated by 3 raters: A, B, C\n",
    "Each row represents a proposal, each column a rater\n",
    "1 for \"Yes\", 0 for \"No\"\n",
    "\"\"\"\n",
    "ratings = np.array([\n",
    "    [1, 1, 0], [1, 0, 1], [0, 1, 1], [1, 1, 1], [0, 0, 0], \n",
    "    [1, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 1, 1],\n",
    "    # ... (we can add more rows)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbb10297-2712-4d64-8f93-a0580bee7e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0],\n",
       "       [1, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the ratings matrix\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe685e-fb65-43d2-a6f9-206417c386a6",
   "metadata": {},
   "source": [
    "The below cell calculates the occurance of each rating per proposal and creates an array where each row represents the count of \"No\" and \"Yes\" ratings for each proposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f829c931-6d71-4e3c-9c0c-3d11d3e2ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts = np.apply_along_axis(lambda x: np.bincount(x, minlength=2), axis=1, arr=ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aaacb3bc-537b-4005-bb46-6e9538beb28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 2],\n",
       "       [1, 2],\n",
       "       [0, 3],\n",
       "       [3, 0],\n",
       "       [1, 2],\n",
       "       [2, 1],\n",
       "       [2, 1],\n",
       "       [2, 1],\n",
       "       [0, 3]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdacf64-335c-4180-8f75-5ac523a47943",
   "metadata": {},
   "source": [
    "As we can see, the first row in the array is [1, 2] which implies there are two 1's and a single 0. Similarly, the fourth row is [0, 3] which means there are 3 1's and no any 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fb78698-b1c0-4a1b-b141-1b0f9f3df793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Fleiss' Kappa.\n",
    "fleiss_score = fleiss_kappa(ratings_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d33e023-d461-4850-bef8-ab592ce09241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fleiss' Kappa: 0.050\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Fleiss' Kappa: {fleiss_score:.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f687ea9-e1dd-4017-a684-3980ad37cc55",
   "metadata": {},
   "source": [
    "This result falls into the range of 0.40 - 0.60, indicating **moderate agreement** between the raters. This suggests a reasonable level of consistency, but there is still some variability in the ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0e015-300b-4a43-bec3-437c49cc8af5",
   "metadata": {},
   "source": [
    "3)  **Intra-Class Correlation (ICC)**:\n",
    "\t- It is commonly used to quantify the degree to which individuals with a fixed degree of relatedness (eg: full siblings) resemble each other in terms of quantitative trait ([[heritability]]).\n",
    "\t- It describes how strongly units in the same group resemble each other.\n",
    "\t- It is useful when ratings are numerical (continuous) rather than categorical.\n",
    "\t- Example: In a psychological study, multiple raters might score/rate the same set of participants on a psychological scale. ICC would measure the consistency of these ratings or scores.\n",
    "\t- Formula:\n",
    "\t$$\n",
    "\t{\\displaystyle Y_{ij}=\\mu +\\alpha _{j}+\\varepsilon _{ij},}\n",
    "\t$$\n",
    "\t\twhere, <br>\n",
    "\t\t$Y_{ij}$ is  the $i^{th}$ observation in the $j^{th}$ group <br>\n",
    "\t\t$μ$ = an unobserved overall mean <br>\n",
    "\t\t$α_{j}$ = an unobserved random effect shared by all values in group $j$ <br>\n",
    "\t\t$ε_{ij}$ = an unobserved noise term\n",
    "\n",
    "\t\t**Note**: For the model to be identified, the $α_{j}$ and $ε_{ij}$ are assumed to have expected value zero and to be uncorrelated with each other. Also, the $α_{j}$ are assumed to be identically distributed, and the $ε_{ij}$ are assumed to be identically distributed. The variance of $α_{j}$ is denoted $σ_{α}^2$ and the variance of $ε_{ij}$ is denoted $σ_{ε}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f88244-f3d8-4e67-a1c7-1c06a119df66",
   "metadata": {},
   "source": [
    "#### Example Data:\n",
    "\n",
    "Let's assume we have continuous ratings by 3 raters for 10 proposals. So let's calculate the intra-class correlation!\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Create a dataset (dummy) where each row should represent a proposal and each column represents a rater.\n",
    "- Compute the Intra-class correlation using `pingouin` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "442170e3-787d-473f-a79d-0127f0811f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pingouin import intraclass_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8395c93d-c294-4090-a1f1-3687e47551cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset\n",
    "data = {\n",
    "    'proposal': np.tile(np.arange(10), 3),\n",
    "    'rater': np.repeat(['Rater1', 'Rater2', 'Rater3'], 10),\n",
    "    'rating': [4.2, 3.8, 5.0, 4.5, 4.1, 4.7, 3.9, 4.3, 4.4, 4.8,\n",
    "               4.1, 3.7, 4.8, 4.6, 4.0, 4.8, 3.8, 4.2, 4.3, 4.7,\n",
    "               4.3, 3.9, 4.9, 4.4, 4.2, 4.6, 4.0, 4.4, 4.5, 4.9]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9938313a-7d26-471e-926f-2832424023aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proposal</th>\n",
       "      <th>rater</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Rater1</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Rater1</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Rater1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Rater1</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Rater1</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   proposal   rater  rating\n",
       "0         0  Rater1     4.2\n",
       "1         1  Rater1     3.8\n",
       "2         2  Rater1     5.0\n",
       "3         3  Rater1     4.5\n",
       "4         4  Rater1     4.1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95370d-2fda-486b-aa51-ffe09b3d82ce",
   "metadata": {},
   "source": [
    "As we can observe in the above table, each row represents a proposal and raters are represented by the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "645964c5-29e2-4976-9b58-07f4364ec2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ICC\n",
    "icc = intraclass_corr(data=df, targets='proposal', raters='rater', ratings='rating')\n",
    "icc_df = pd.DataFrame(icc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dbafe512-7393-493f-be81-c837ff2e16e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Description</th>\n",
       "      <th>ICC</th>\n",
       "      <th>F</th>\n",
       "      <th>df1</th>\n",
       "      <th>df2</th>\n",
       "      <th>pval</th>\n",
       "      <th>CI95%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ICC1</td>\n",
       "      <td>Single raters absolute</td>\n",
       "      <td>0.930982</td>\n",
       "      <td>41.466667</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>5.374008e-11</td>\n",
       "      <td>[0.82, 0.98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ICC2</td>\n",
       "      <td>Single random raters</td>\n",
       "      <td>0.931350</td>\n",
       "      <td>54.086957</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>3.370376e-11</td>\n",
       "      <td>[0.8, 0.98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ICC3</td>\n",
       "      <td>Single fixed raters</td>\n",
       "      <td>0.946512</td>\n",
       "      <td>54.086957</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>3.370376e-11</td>\n",
       "      <td>[0.85, 0.99]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ICC1k</td>\n",
       "      <td>Average raters absolute</td>\n",
       "      <td>0.975884</td>\n",
       "      <td>41.466667</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>5.374008e-11</td>\n",
       "      <td>[0.93, 0.99]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICC2k</td>\n",
       "      <td>Average random raters</td>\n",
       "      <td>0.976019</td>\n",
       "      <td>54.086957</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>3.370376e-11</td>\n",
       "      <td>[0.92, 0.99]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ICC3k</td>\n",
       "      <td>Average fixed raters</td>\n",
       "      <td>0.981511</td>\n",
       "      <td>54.086957</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>3.370376e-11</td>\n",
       "      <td>[0.95, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Type              Description       ICC          F  df1  df2  \\\n",
       "0   ICC1   Single raters absolute  0.930982  41.466667    9   20   \n",
       "1   ICC2     Single random raters  0.931350  54.086957    9   18   \n",
       "2   ICC3      Single fixed raters  0.946512  54.086957    9   18   \n",
       "3  ICC1k  Average raters absolute  0.975884  41.466667    9   20   \n",
       "4  ICC2k    Average random raters  0.976019  54.086957    9   18   \n",
       "5  ICC3k     Average fixed raters  0.981511  54.086957    9   18   \n",
       "\n",
       "           pval         CI95%  \n",
       "0  5.374008e-11  [0.82, 0.98]  \n",
       "1  3.370376e-11   [0.8, 0.98]  \n",
       "2  3.370376e-11  [0.85, 0.99]  \n",
       "3  5.374008e-11  [0.93, 0.99]  \n",
       "4  3.370376e-11  [0.92, 0.99]  \n",
       "5  3.370376e-11   [0.95, 1.0]  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921bc4f-bb73-4ccd-a3a5-50cbd37adbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
